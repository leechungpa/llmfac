### model
model_name_or_path: Qwen/Qwen2.5-7B-Instruct # Qwen/Qwen3-4B-Instruct-2507  Qwen/Qwen3-4B Qwen/Qwen3-8B

### method
stage: sft
do_train: true
finetuning_type: lora
# lora_rank: 8
# lora_target: q_proj,k_proj
# deepspeed: examples/deepspeed/ds_z3_config.json  # choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]


### dataset
dataset: mmlu_train_cot_s0
template: qwen
cutoff_len: 2048 # increase this for cot
max_samples: 10000
preprocessing_num_workers: 16
dataloader_num_workers: 4


### output
logging_steps: 10
save_steps: 100
plot_loss: true
# overwrite_output_dir: true
save_only_model: false
report_to: wandb  # choices: [none, wandb, tensorboard, swanlab, mlflow]

# If mix
# mix_strategy: interleave_under

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8 # effective batch size = 8
learning_rate: 1.0e-5
# num_train_epochs: 5
warmup_ratio: 0.0
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

## eval
eval_dataset: mmlu_test_s0
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 10

